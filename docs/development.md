# Development Setup

## Prerequisites

* [Docker](https://docs.docker.com/get-started/)
* [Docker Compose](https://docs.docker.com/compose/install/) (does not need to be installed separately if using Docker Desktop)
* Python 3.9
  * Optional: [pyenv](https://github.com/pyenv/pyenv) for managing Python versions
* Node.js >= 20
  * Optional: [nvm](https://github.com/nvm-sh/nvm) for managing Node.js versions
* Yarn 1.x
  * Recommended: install via `corepack`:
    ```shell
    corepack enable
    corepack install --global yarn@1
    ```
* A local clone of the `nmdc-server` repository

## Configuration

Start by copying the example environment configuration file.

```bash
cp .env.example .env
```

See `nmdc_server/config.py` for all configuration variables.  Variable names in `.env` should be all uppercase and prefixed with `NMDC_`.

Follow the steps below to configure the necessary settings.

### Authentication

1. If necessary, create an ORCID account at [orcid.org](https://orcid.org).
2. Once logged in to ORCID, create an Application via the ORCID [developer tools](https://orcid.org/developer-tools) page.
    - In the "Application details" section:
        - Set the **Application name** to `NMDC Portal`
        - Set the **Application URL** to `https://microbiomedata.org/`
        - Set the **Application description** to `NMDC Portal`
    - In the "Redirect URIs" section:
      - Add an entry for `http://127.0.0.1:8000`
      > **Note**: Our production Redirect URIs are listed
          [here](https://github.com/microbiomedata/infra-admin/blob/main/orcid/README.md#redirect-uris).
    - You will use the resulting **Client ID** and **Client Secret** in the next step.
3. Populate the following variables in `.env`.
   ```bash
   NMDC_ORCID_CLIENT_ID=changeme
   NMDC_ORCID_CLIENT_SECRET=changeme
   ```
4. Populate the below variables in `.env`. Values for these should be generated by running `openssl rand -hex 32`. You will have to run the command once for each variable (i.e. twice total).
   ```bash
   NMDC_SESSION_SECRET_KEY=changeme
   NMDC_API_JWT_SECRET=changeme
   ```

### NERSC Credentials

To load production data or to run an ingest locally, you will need NERSC credentials.

1. If necessary, create a new account by following the instructions at https://docs.nersc.gov/accounts.
2. Populate the following variable in `.env`.
    ```bash
    NERSC_USER=changeme
    ```
3. Install the `sshproxy` tool by following the instructions at https://docs.nersc.gov/connect/mfa/#sshproxy.

### MongoDB Credentials

In order to connect to the dev or prod MongoDB instances for ingest, you will need credentials to connect to them. If you do not have credentials, ask a team member to either create accounts for you or to provide you with the generic `org.microbiomedata.data_reader` credentials. Then add the credentials to your `.env` file.

```bash
NMDC_MONGO_USER=changeme
NMDC_MONGO_PASSWORD=changeme
```

### Google Cloud Storage

Google Cloud Storage (GCS) is used to store images associated with Submission Portal submissions. By default, local development uses a local mock GCS server. This is controlled by the `NMDC_GCS_USE_FAKE` variable in `.env`. If you want to use the real GCS server, set this variable to `false`.

Whether you use the real or fake GCS server, you will need to set up authentication. The recommended way to do this is to use Application Default Credentials (ADC) with service account impersonation. Service account impersonation is required for generating signed URLs for uploading/downloading images directly to/from GCS.

1. Ask a team member with the necessary GCS permissions to associate your Google Cloud account with the NMDC Google Cloud project and service account.
   <details>
   <summary>Show/hide instructions for team member</summary>

   These instructions are **for the team member** that has necessary GCS permissions to associate your (i.e. the "developer's") Google Cloud account with the NMDC Google Cloud project and service account.

   1. In a web browser, sign into the [Google Cloud console](https://console.cloud.google.com) and go to the "`NMDC`" project (if not already there) in the "`lbl.gov`" organization.
   2. Go to the "IAM & Admin" > "IAM" section of the Google Cloud console.
   3. On the "Allow" tab, go to the "View by principals" sub-tab.
   4. **If the developer is already listed as a principal**, perform the following sub-steps to confirm they have sufficient roles; then **skip step 5 below**.
      1. Click the pencil icon next to their name.
      2. In the "Assign roles" section of the form, select the following two roles:
         - `Service Account Token Creator`
         - `View Service Accounts`
      3. Click the "Save" button.
   5. **If the developer is _not_ already listed as a principal**, perform the following sub-steps to add the developer as a principal having sufficient roles.
      1. Click the "Grant access" button.
      2. In the "New principals" field, enter the email address of the developer's Google account.
      3. In the "Assign roles" section of the form, select the following two roles:
         - `Service Account Token Creator`
         - `View Service Accounts`
      4. Click the "Save" button.
   6. Send the service account email address to the developer.
      1. Back on the "View by principals" sub-tab, locate the row whose "Name" value says "`Mister Bucket`".
      2. On that row, copy the email address in the "Principal" column. It's the same email address every timeâ€”we have just opted not to include it in these public docs.
      3. Send that email address to the developer. You can include the following introduction for context if you want (optional):
         ```
         Here is the service account email address you can use when setting up Application Default Credentials (ADC):
         ```
   </details>

2. Install the Google Cloud Command Line Interface (CLI) by following the instructions at https://cloud.google.com/sdk/docs/install.
3. Run the following command to set up Application Default Credentials (ADC):
    ```bash
    gcloud auth application-default login --impersonate-service-account <service account email will be provided by team member>
    ```

You also must generate a local object name prefix. This prefix is used to differentiate which system uploaded to the shared GCS bucket. Local development systems should use the prefix `local_<random_suffix>`.

1. Generate a random suffix using the following command:
    ```bash
    openssl rand -hex 4
    ```
2. Set the `NMDC_GCS_OBJECT_NAME_PREFIX` variable in `.env` to `local_<random_suffix>`.

    ```bash
   NMDC_GCS_OBJECT_NAME_PREFIX=local_1234abcd  # replace 1234abcd with your random suffix
   ```


## Load production data

The `nmdc-server` CLI has a `load-db` subcommand which populates your local database using a nightly production backup. These backups are stored on NERSC. You must have NERSC credentials to use this subcommand.

First use NERSC's `sshproxy` [tool](https://docs.nersc.gov/connect/mfa/#sshproxy) to generate an ssh key if you haven't done so in the last 24 hours.

```bash
sshproxy.sh -u $NERSC_USER
```

Then run the `load-db` subcommand from a `backend` container, mounting the ssh key.

```bash
docker compose run \
  --rm \
  -v ~/.ssh/nersc:/tmp/nersc \
  backend \
  nmdc-server load-db -u $NERSC_USER
```

To see all CLI options run:

```bash
nmdc-server load-db --help
```

**Note**: if you already have a local database set up, the first time you attempt to load from a production backup you may see an error about a missing `nmdc_data_reader` role. If you see this error, run the following command to remove existing docker volumes:

```bash
docker compose down -v
```

This should only need to be done once. When the `db` service starts up again (including via running the `load-db` command), the necessary roles and databases will be created automatically.

<details>
<summary><b>Don't have a NERSC account?</b></summary>
If you're an NMDC team member, but don't have a NERSC account yet: talk to your team lead about getting a NERSC account, specifically one that has access to NMDC's project files. The process takes about a week. Docs: https://docs.nersc.gov/accounts/

If that is not an option for you:

1. Ask an NMDC team member with NERSC access to get a recent production backup for you. This will come in the form of a `.dump` file. Save the file locally.
2. Bring your local database up
    ```bash
    docker compose up db -d
    ```
3. Load data from the `.dump` file into the running database
    ```bash
    docker compose run --rm \
        -v <absolute path to .dump file>:/tmp/backup.dump \
       db pg_restore --dbname postgresql://postgres:postgres@host.docker.internal:5432/nmdc_a --clean --if-exists --verbose --single-transaction /tmp/backup.dump
    ```
</details>

## Installing dependencies locally

Although the project is designed to be run in Docker, having the dependencies installed locally can be useful for development (e.g. providing code completion in your editor, running tests, etc.).

### Backend dependencies

1. If necessary, create a new virtual environment.
    ```bash
    python -m venv .venv
    ````
2. Activate your virtual environment and install the backend dependencies.
    ```bash
    source .venv/bin/activate
    pip install -e .
    ```

### Frontend dependencies

1. Install the frontend dependencies.
    ```bash
    cd web
    yarn
    ```

## Running the server

Run the full stack via Docker Compose:

<!-- TODO: Consider adding `--build` to this command so that Docker Compose builds
           the containers, rather than pulling from from GHCR (unless you
           want to use the versions that happen to currently be on GHCR).
           This has to do with the fact that the `docker-compose.yml` file
           contains service specs having both an `image` and `build` section. -->

```bash
docker compose up -d
```
> The `-d` is short for `--detach` and makes it so the container logs (i.e. STDOUT and STDERR streams) _don't_ take over your shell, causing you to have to open up a new shell in order to run more commands.

Troubleshooting: If the building of one of the services fails with an error citing networking timeouts, _but_ the building of _other_ services completes successfully, we recommend you retry building only the service that failed, by itself. Our thinking is that there will be less demand on your network that way. You can do that via `$ docker compose build {service_name}` (e.g. `$ docker compose build web`).

View the main application at `http://127.0.0.1:8080/` and the API documentation page at `http://127.0.0.1:8080/api/docs`.

Changing Python files in the `nmdc_server` directory will automatically reload the server.

To stop the server:

```bash
docker compose down
```

If you add or modify project dependencies, you may need to rebuild the containers:

```bash
docker compose up --build -d
```

### Running with frontend development server

If you are modifying files in the `web` directory, additionally run the frontend development server to enable hot reloading in your browser:

```bash
cd web
yarn serve
```

<details>
<summary>Running yarn via npx?</summary>

When you run `$ npx yarn serve` while using a Node.js version newer than 17, the frontend development server may fail to start and may, instead, display the error code "`ERR_OSSL_EVP_UNSUPPORTED`".

You can work around that error by prefixing the command with "`NODE_OPTIONS=--openssl-legacy-provider`", as explained [here](https://stackoverflow.com/a/70582385) and shown below:

```bash
NODE_OPTIONS=--openssl-legacy-provider npx yarn serve
```
</details>

View the main application at `http://127.0.0.1:8081/`. Changes to files in the `web` directory will automatically trigger a reload in your browser.

> **Note**: An instance of the frontend application will continue to be served via Docker Compose on port `8080`, but that instance will not pick up changes to the `web` directory automatically. Be aware of which port you are accessing when doing frontend development.

### Why not `localhost`?

It is recommended to use `127.0.0.1` instead of `localhost` for local development. This is because `localhost` is **not** allowed as a redirect URI for an ORCID client. The workaround is to register `127.0.0.1` as a redirect URI with ORCID and to use subsequently visit `127.0.0.1` for local testing.

## Running ingest

> **Note**: This is not generally required unless you are specifically working on the ingest code. If you are working on the web application, simply [loading from a recent production backup](#load-production-data) is sufficient.

1. Ensure that you have completed the sections above about configuring your [NERSC credentials](#nersc-credentials) and [MongoDB credentials](#mongodb-credentials).
2. Obtain a new SSH key from NERSC if you haven't done so in the last 24 hours.
    ```bash
    sshproxy.sh -u $NERSC_USER
    ```
3. Set up an active SSH tunnel to the dev and production MongoDB instances if you do not already have one established.
    ```bash
    ssh \
      -L 37018:mongo-loadbalancer.nmdc-dev.production.svc.spin.nersc.org:27017 \
      -L 37019:mongo-loadbalancer.nmdc.production.svc.spin.nersc.org:27017 \
      -o ServerAliveInterval=60 \
      -f \
      -N \
      -l $NERSC_USER \
      -i ~/.ssh/nersc \
      dtn01.nersc.gov
    ```
    > That command will set up SSH port forwarding such that your computer can access the dev MongoDB server at `localhost:37018` and the prod MongoDB server at `localhost:37019`.

    > From within a Docker container `host.docker.internal` can be used to access the `localhost` of your computer. When ingesting from the dev or prod MongoDB instances, be sure to set `NMDC_MONGO_HOST=host.docker.internal` in your `.env` file.

    > See https://github.com/microbiomedata/infra-admin/blob/main/mongodb/connection-guide.md (internal) for more information on connecting to the MongoDB instances.
4. Create a local copy of ingest support files:
    ```bash
    mkdir -p data && scp \
      -r \
      -i ~/.ssh/nersc \
      "$NERSC_USER@dtn01.nersc.gov:/global/cfs/cdirs/m3408/ingest" \
      data
    ```
5. Run the ingest command:
    ```bash
    docker-compose run backend nmdc-server ingest -vv --function-limit 100
    ```

    > **Note**: The `--function-limit` flag is optional. It is used to reduce the time that the ingest takes by limiting the number of certain types of objects loaded. This can be useful for testing purposes. For more information on options run `nmdc-server ingest --help`.

## Testing

```bash
tox
```

In order for the `py312` test suite to run properly, it needs to be able to communicate with a running `postgres` server and the fake GCS service. You can use the docker configuration to get these services up and running:

```bash
docker compose up db storage
```

You'll also need to set environment variables so the tests know where these resources can be found. If you're running the services via `docker compose`, then you can use the following values:

```bash
export NMDC_TESTING_DATABASE_URI=postgresql://postgres:postgres@localhost:5432/nmdc_testing
export NMDC_GCS_FAKE_API_ENDPOINT=http://localhost:4443
```

## Generating new migrations

```bash
# Autogenerate a migration diff from the current HEAD
docker-compose run backend alembic -c nmdc_server/alembic.ini revision --autogenerate
```

In order to generate a migration, your database state should match HEAD.  If you started the server from a totally empty database, then the default behavior is to ignore migration scripts and set up the database to match `models.py`.  Before you can generate a migration, you need to reset your database to match HEAD.

```bash
# Destroy everything.  You'll lose your data!
docker-compose down -v
docker-compose up -d db
# Create the database
docker-compose run backend psql -c "create database nmdc_a;" -d postgres
# Run migrations to HEAD
docker-compose run backend alembic -c nmdc_server/alembic.ini upgrade head
# Autogenerate a migration diff from the current HEAD
docker-compose run backend alembic -c nmdc_server/alembic.ini revision --autogenerate
```

## Developing with the shell

A handy IPython shell is provided with some commonly used symbols automatically
imported, and `autoreload 2` enabled. To run it:

```bash
docker-compose run --rm backend nmdc-server shell
```

You can also pass `--print-sql` to output all SQL queries. Note that if you have SQL logging enabled via the `NMDC_PRINT_SQL` environment variable, you will not need to pass `--print-sql` to the command.

The shell can be very helpful for gaining an understanding of how the data model and query/filter classes work. Here are some prompts you can start with to explore the system.

### Creating a `Session`

You'll need a SQLAlchemy `session` object to perform queries. The functions needed to create a session (either for the "active" database or the "ingest" database) are included in the auto-import statements. To create a session that can talk to the "active" database, you can run:

```python
db = SessionLocal()
```


### Querying `Biosample`

To create a basic SQLAlchemy `select` object (which can be executed against the session), run:

```python
select_statement = select(Biosample)
```

To actually execute the statement, you'll need to run:

```python
result = db.execute(select_statement)
one_biosample = result.scalars().first()
```

Note that `result.scalars()` returns an instance of [ScalarResult](https://docs.sqlalchemy.org/en/14/core/connections.html#sqlalchemy.engine.ScalarResult). This can be used to get ORM objects. The above snippet uses the `first` method to retrieve a single biosample, which is an instace of `nmdc_server.models.Biosample`. For more details on using SQLAlchemy to explore results, see the official documentation.

To see an example of how relationships work in SQLAlchemy, use the Biosample retrieved abolve and run:

```python
one_biosample.omics_processing
```

You can analyze the SQL output to better understand how this relationship works in SQLAlchemy, and what the engine needs to do to get from one entity (in this case, Biosample) to another (OmicsProcssing). This query in particular also reveals the association table between those two.

### Use of `selectinload`

Using the shell like this is a great way to explore SQLAlchemy ideas and concepts. Having this tool is important for analyzing performance and optimizing queries during search. Here is an example that illustrates how SQLAlchemy's `selectinload` can reduce the number of queries the system needs to run.

```python
from sqlalchem.orm import selectinload

db = SessionLocal()
biosample_query = db.query(Biosample).options(selectinload(Biosample.omics_processing))
one_biosample = biosample_query[0]
one_biosample.omics_processing
```

Executing the above line by line with SQL logging turned on will reveal that `selectinload` will query for the biosample's omics processing data before the property is accessed. The final line, `one_biosample.omics_processing` does not emit a SQL statement. That related information has already been retrieved, thanks to `selectinload`.

### Simple Exploration of QuerySchema classes

The following example illustrates how the `QuerySchema` and `ConditionSchema` classes are used to generate queries for searches from the data portal. `QuerySchema` classes contain important methods like `execute`, `query`, and `facet`. `ConditionSchema` classes model individual conditions (usually chosen by the user in the Data Portal's search sidebar). This example uses the `SimpleConditionSchema` class, which has the following properties:

1. `op`: The operation (equals, greater than, etc.) to use. Expects a value from the `Operation` enum in `query.py`
2. `table`: The table that contains the value we're searching on
3. `field`: The column (or JSONb property within the `annotations` column) to search on
4. `value`: The value used to search against.

This structure mirrors the individual `conditions` sent to the API's `search/` endpoints.

You can use these classes to run searches just like the API does, follow these steps in your shell to walk through a simple example.

```python
filter_conditions = [SimpleConditionSchema(table='biosample', field='latitude', value='45', op=Operation.greater)]
```

The `QuerySchema` classes expect an list of conditions, which is why we're wrapping our `SimpleConditionSchema` object in a list here.

```python
study_query_schema = StudyQuerySchema(conditions=conditions)
```

Instantiate the `StudyQuerySchema` class with our condition. We're going to use this class to query the database for all studies that have related biosamples in the latitude range specified by the condition above.

```python
db = SessionLocal()
result = study_query_schema.execute(db)
one_study = result[0]
```

The `execute` method will construct the query based on the conditions and execute it against the given SQLAlchemy `session` (in this case, the "active" database). Evaluating the result (in this case, retrieving the study at index `0`) will cause the SQL to run and be logged.

As an exercise, try running the same search with an instance of the `BiosampleQuerySchema` class. Compare the SQL generated in both cases. It is quite different.

Child classes of the `BaseQuerySchema` can override the default behavior. For example, study results need to inject counts of data types into the results.

For more on how `Conditions` are tranformed into `Filters`, see the `BaseQuerySchema` class, and the `filters.py` module.

## Google Analytics

The frontend Vue app is configured to work with Google Analytics. There are two Google Analytics "properties" for the NMDC Data Portal. One is for the production site. One is for the dev site as well as local development. Each GA property has its own GA ID associated with it. This ID gets pulled into the view app from an environment variable called `VUE_APP_NMDC_GOOGLE_ANALYTICS_ID`. You can set this inside a `.env` file inside the `web` directory (note this is separate from the `.env` file at the root of the project).
